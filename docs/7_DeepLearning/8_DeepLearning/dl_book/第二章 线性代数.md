# 第二章 线性代数
第二章主要是介绍了线性代数的一些基础知识

这里补充几点吧

## 特征分解

每个矩阵可以视为多个列向量组成的一个向量空间/或者是一个线性变化(一个矩阵乘以一个向量后相当于是对向量做了一个线性变换)。而矩阵的分解，相当于去提取这个向量空间最重要的特征。

(1)首先是特征值和特征向量
若**方阵**A满足 $Av=\lambda v$,则$v$称为矩阵A的特征向量，$\lambda$称为对应的特征值。

**特征分解**
$$A=V \Lambda V^{-1}$$
V是其特征向量构成的矩阵，$\Lambda$是特征值构成的对角矩阵


性质：

(1) 实对称矩阵A都可以分解为$A=Q\Lambda Q^{-1}$，Q为正交矩阵

(2)用来计算矩阵的逆。若$A=Q \Lambda Q^{-1}$，则$A^{-1}=Q \Lambda^{-1}Q^{-1}$

## 奇异值分解
特征分解对于提取矩阵的特征根是很好的方法，但是他只能针对**方阵**操作。当A不是方阵时候，就需要用到SVD
$$A_{n*m}=U_{n*n}\Sigma_{n*m} V^{T}_{m*m}$$

其中，U称为左奇异向量，V称为右奇异向量


$AA^T=UDV^TVDU^T=UD^2U^T$
$A^TA=VDU^T UDV^T=VD^2V^T$
即：
A的左奇异向量是$AA^T$的特征向量，A的右奇异向量是$A^TA$的特征向量，A的奇异值是$AA^T$也是$A^TA$的特征之爱的平方根。


> 对比：
> 特征分解是有限制的，比如矩阵必须是方阵

### SVD与PCA的关系

* [ ] 查一下之前多元的变换原理

PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。

### SVD与潜在语义检索LSI
 

## 参考资料

http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html



