# 【NG-DL】course2_week1 优化网络参数

## 1.基本概念
很多优化参数：
网络层数，隐层数，学习率，激活函数。。。


* 训练集/验证集/测试集

* 偏差， 训练集误差
* 方差


若有高bias： 尝试新的网络，更大网络

高方差： 获取更多数据，或者正则化，更合适的网络机构

bias和varaince的平衡
一般来说更大的网络能减小偏差，更多的数据能减小方差。

## 2.正则化-L2
以逻辑回归为例

$J(w,b)=\frac{1}{m}\sum L(\hat y_i, y_i)+\frac{\lambda}{2m}|w|^2_2$L2惩罚，避免过拟合，导致高方差


类似的神经网络在优化函数上加上

$J(w^{[1]},b^{[1]},...w^{[l]},b^{[l]})=\frac{1}{m}\sum L(\hat y_i, y_i)+\frac{\lambda}{2m}\sum||w^{[l]}||^2_2$

其中$|w^{[l]}|^2$F范数

* Ng.当$\lambda$较大时候,w会较小，sigmoid接近线性，降低方差。
![](../../../Draft/media/15055593941505/15103891828021.jpg)

### 正则化-dropout(随机失活)

每次对网络中的节点设置一个随机消失的概率p，即每个节点可能在也可能不在网络中，不在的时候即相当于该节点的数值置为0.

dropout与l2类似都会shrink权重。

### early stoping


## 3.优化

### 归一化
归一化输入=> 均值0，方差1
* 零均值化
* 归一化方差


![](../../../Draft/media/15055593941505/15103915636569.jpg)

### 梯度消失与梯度爆炸

当梯度增长很大或者很微弱时，会导致优化出问题或者很慢。





