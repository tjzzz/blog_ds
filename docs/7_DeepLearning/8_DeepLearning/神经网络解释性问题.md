# 神经网络解释性问题


https://ask.hellobi.com/blog/zlx19930503/10578


但是深度学习是一个黑箱。我第一次听说它时，就对其工作原理非常费解。几年过去了，我仍然在探索合理的答案。尝试解释现代神经网络很难，但是至关重要。如果我们打算依赖深度学习制造新的 AI、处理敏感的用户数据，或者开药，那么我们必须理解这些模型的工作原理。


很幸运，学界人士也提出了很多对深度学习的理解。以下是几个近期论文示例：


## Grad-Cam（Selvaraju et. al. 2017）


使用最后卷积层的梯度生成热力图，突出显示输入图像中的重要像素用于分类。

## LIME（Ribeiro et. al. 2016）

使用稀疏线性模型（可轻松识别重要特征）逼近 DNN 的预测。

论文地址：https://arxiv.org/abs/1602.04938

一些中文解读：https://www.oreilly.com.cn/ideas/?p=563；
http://geek.csdn.net/news/detail/66259

一个例子
http://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html

https://www.jiqizhixin.com/articles/2017-12-20-2



## 特征可视化（Olah 2017）
对于带有随机噪声的图像，优化像素来激活训练的 DNN 中的特定神经元，进而可视化神经元学到的内容。
## Loss Landscape（Li et. al. 2017）

可视化 DNN 尝试最小化的非凸损失函数，查看架构／参数如何影响损失情况。


## 关于xgboost
R: https://github.com/AppliedDataSciencePartners/xgboostExplainer
python https://github.com/gameofdimension/xgboost_explainer

