# FM 系列
## 1.FM
背景

一般的线性模型 $y=w_0 + \sum w_i x_i$， 当考虑了特征之间的相关性即交互效应(这里以二阶为例)后模型形式是：
$$y=w_0 + \sum w_i x_i + \sum_1^{n-1}\sum_{i+1}^n w_{ij}x_ix_j$$

这里n表示特征的个数


模型如何求解呢？
如果把$x_ix_j$看做一个新的变量，相当于整体上还是一个线性回归模型，似乎可以用正常的线性回归的办法进行求解。
但是这里未知的要求解的参数有n(n+1)/2个，很多时候特征中会有一些分类型的变量，当通过哑变量的方式引入模型中的时候，特征数目k会非常大，从而导致要求解的参数个数较大，并且数据特别稀疏。另一方面，分类特征就到时候，$x_ix_j$是非零的比例较小，从而导致$w$无法计算出来。

> 引入其他辅助信息

我们可以引入一个辅助向量 对于$w_{ij}$形成的对称矩阵W，可以进行分解$W=V^TV$

将模型改成如下形式
$$y=w_0 + \sum w_i x_i + \sum_1^{n-1}\sum_{i+1}^n <v_i, v_j>x_ix_j$$

* 从形式上来看，我们知道$w_ij$的矩阵是对称的，其形式上和协差阵$<v_i, v_j>$很类似

$v_i$相当于是第i维隐向量，假设其维度是k<<n, 这样模型的参数就由原来的n(n+1)/2 变为了nk

FM模型可以看做是SVD分解的一种泛化形式



## FFM
引入了特征域的概念。
不光认为特征与特征之间有潜在的联系，还认为特征与特征的类型之间也有联系。

简单举例：
比如3个特征a, b, c, 在FM中 (a,c), (a,b) 对a来说所使用的隐向量是一样的，但是在FFM中是不一样的 





## wide&deep

google与2016提出的wide&deep, 
- wide: 主要作用是让模型具有较强的”记忆能力“，学习历史数据中的共线能力
- deep: 主要是让模型具有”泛化能力“，“泛化能力”可以被理解为模型传递特征的相关性，以及发掘稀疏甚至从未 出现过的稀有特征与最终标签相关性的能力。

![](../../Draft/media/Pasted%20image%2020220526154121.png)



### deepFM
![](../../Draft/media/Pasted%20image%2020220526154928.png)

对 Deep 模型的改进之处在于，它用FM模型替换了原来的wide部分，加强了浅层网络部分特征组合的能力。如图 -19 所示 ，左边的 部分与右边的深度神经网络部分共享相同的 Embedding 层。左 侧的 FM 部分对不同的特征域的 进行了两两交叉 ,也就是将 Embedding 向量当作原 中的特征隐向量。最后将 FM 的输出与 Deep 部分的输出一同输 入最后的输出层，参与最后的目标拟合。

## NFM
经典 FM 的数学形式已经由（ 式 3-10 ) 给出，在数学形式上，NFM 模型的

主要思路是用一个表达能力更强的函数替代原 FM 中二阶隐向量内积的部分
![](../../Draft/media/Pasted%20image%2020220526155438.png)







## 参考资料
简书-石晓文 https://www.jianshu.com/p/152ae633fb00
美团技术博客 https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html


